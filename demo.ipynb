{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2 with plot\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# ... [Keep the existing functions: mean_absolute_percentage_error, load_and_clean_data, add_lag_and_rolling_features] ...\n",
    "\n",
    "# Function to calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_idx = y_true != 0\n",
    "    y_true_non_zero = y_true[non_zero_idx]\n",
    "    y_pred_non_zero = y_pred[non_zero_idx]\n",
    "    if len(y_true_non_zero) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true_non_zero - y_pred_non_zero) / y_true_non_zero)) * 100\n",
    "\n",
    "# Load and clean data\n",
    "def load_and_clean_data(file_path):\n",
    "    data = pd.read_excel(file_path, parse_dates=['DateAndHour'])\n",
    "    \n",
    "    # Feature engineering: Date and Time\n",
    "    data['Hour'] = data['DateAndHour'].dt.hour\n",
    "    data['DayOfWeek'] = data['DateAndHour'].dt.dayofweek\n",
    "    data['Month'] = data['DateAndHour'].dt.month\n",
    "    \n",
    "    # Remove rows with missing Temperature or Load_data\n",
    "    data = data.dropna(subset=['Temperature', 'Load_data'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Adding lag, rolling features, and temperature bins\n",
    "def add_lag_and_rolling_features(data, lags=[1, 24, 168], window=3):\n",
    "    for lag in lags:\n",
    "        data[f'Load_data_lag_{lag}'] = data['Load_data'].shift(lag)\n",
    "        data[f'Temperature_lag_{lag}'] = data['Temperature'].shift(lag)\n",
    "    \n",
    "    # Rolling features for load data (mean and standard deviation)\n",
    "    data[f'Load_data_rolling_{window}'] = data['Load_data'].rolling(window=window).mean()\n",
    "    data[f'Load_data_rolling_std_{window}'] = data['Load_data'].rolling(window=window).std()\n",
    "    \n",
    "    # Adding temperature change feature\n",
    "    data['Temperature_change'] = data['Temperature'].diff()\n",
    "    \n",
    "    # Temperature Binning (cold, moderate, hot)\n",
    "    data['Temperature_bins'] = pd.cut(data['Temperature'], bins=[-np.inf, 0, 15, np.inf], labels=['cold', 'moderate', 'hot'])\n",
    "    \n",
    "    # Convert categorical bins to dummy variables\n",
    "    data = pd.get_dummies(data, columns=['Temperature_bins'], drop_first=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load and prepare data\n",
    "data = load_and_clean_data('/kaggle/input/baseline2/Hackathon_Data_Cleaned.xlsx')\n",
    "data = add_lag_and_rolling_features(data, lags=[1, 24, 168], window=3)\n",
    "\n",
    "# Select test dates\n",
    "test_dates = ['2024-02-01', '2024-03-02', '2024-03-26', '2024-03-30']\n",
    "test_mask = data['DateAndHour'].dt.date.astype(str).isin(test_dates)\n",
    "test_data = data[test_mask]\n",
    "train_data = data[~test_mask]\n",
    "\n",
    "# Prepare features and target\n",
    "features = ['Hour', 'DayOfWeek', 'Month', 'Temperature', 'Temperature_change',\n",
    "            'Load_data_lag_1', 'Load_data_lag_24', 'Load_data_lag_168', \n",
    "            'Temperature_lag_1', 'Temperature_lag_24', 'Temperature_lag_168',\n",
    "            'Load_data_rolling_3', 'Load_data_rolling_std_3',\n",
    "            'Temperature_bins_moderate', 'Temperature_bins_hot']\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data['Load_data']\n",
    "X_test = test_data[features]\n",
    "y_test = test_data['Load_data']\n",
    "\n",
    "# Handle missing values and scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train.fillna(X_train.mean())), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test.fillna(X_test.mean())), columns=X_test.columns)\n",
    "\n",
    "# XGBoost model\n",
    "def xgboost_model(X_train, y_train, X_test):\n",
    "    model = xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "# Prophet model\n",
    "def prophet_model(data_train, data_test):\n",
    "    model = Prophet(daily_seasonality=True)\n",
    "    model.add_regressor('Temperature')\n",
    "    model.fit(data_train)\n",
    "    future = data_test[['ds']].copy()\n",
    "    future['Temperature'] = data_test['Temperature'].values\n",
    "    forecast = model.predict(future)\n",
    "    return forecast['yhat'].values\n",
    "\n",
    "# ARIMA model\n",
    "def arima_model(y_train, n_test):\n",
    "    model = ARIMA(y_train, order=(5,1,2))\n",
    "    results = model.fit()\n",
    "    return results.forecast(steps=n_test)\n",
    "\n",
    "# Random Forest model\n",
    "def random_forest_model(X_train, y_train, X_test):\n",
    "    model = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=5, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "# Train and predict\n",
    "xgb_pred = xgboost_model(X_train_scaled, y_train, X_test_scaled)\n",
    "\n",
    "prophet_data_train = train_data[['DateAndHour', 'Load_data', 'Temperature']].rename(columns={'DateAndHour': 'ds', 'Load_data': 'y'})\n",
    "prophet_data_test = test_data[['DateAndHour', 'Load_data', 'Temperature']].rename(columns={'DateAndHour': 'ds', 'Load_data': 'y'})\n",
    "prophet_pred = prophet_model(prophet_data_train, prophet_data_test)\n",
    "\n",
    "rf_pred = random_forest_model(X_train_scaled, y_train, X_test_scaled)\n",
    "arima_pred = arima_model(y_train, len(y_test))\n",
    "\n",
    "# Meta-learner\n",
    "meta_X = np.column_stack((xgb_pred, prophet_pred, rf_pred, arima_pred))\n",
    "meta_learner = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "meta_learner.fit(meta_X, y_test)\n",
    "\n",
    "# Final predictions\n",
    "final_pred = meta_learner.predict(meta_X)\n",
    "\n",
    "# Calculate MAE and MAPE\n",
    "ensemble_mae = mean_absolute_error(y_test, final_pred)\n",
    "ensemble_mape = mean_absolute_percentage_error(y_test, final_pred)\n",
    "\n",
    "print(f\"Ensemble Model MAE: {ensemble_mae:.2f}\")\n",
    "print(f\"Ensemble Model MAPE: {ensemble_mape:.2f}%\")\n",
    "\n",
    "# Append predictions to the original dataset\n",
    "data.loc[test_mask, 'Predicted_Load'] = final_pred\n",
    "\n",
    "# Display results for the test dates\n",
    "for date in test_dates:\n",
    "    print(f\"\\nPredictions for {date}:\")\n",
    "    day_data = data[data['DateAndHour'].dt.date.astype(str) == date]\n",
    "    print(day_data[['DateAndHour', 'Load_data', 'Predicted_Load']])\n",
    "\n",
    "# Optionally, save the updated dataset\n",
    "data.to_csv('load_data_with_predictions.csv', index=False)\n",
    "print(\"\\nUpdated dataset saved as 'load_data_with_predictions.csv'\")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, date in enumerate(test_dates):\n",
    "    plt.subplot(2, 1, i+1)\n",
    "    day_data = data[data['DateAndHour'].dt.date.astype(str) == date]\n",
    "    plt.plot(day_data['DateAndHour'], day_data['Load_data'], label='Actual Load', marker='o')\n",
    "    plt.plot(day_data['DateAndHour'], day_data['Predicted_Load'], label='Predicted Load', marker='x')\n",
    "    plt.title(f'Load Prediction for {date}')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Load')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('load_predictions.png')\n",
    "plt.close()\n",
    "print(\"Prediction plots saved as 'load_predictions.png'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
